{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Experiment with $n$-grams "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we incrementally increase the size of $n$-grams and observe its effects on classification accuracy, dimensionality, training time and other related metrics. \n",
      "\n",
      "There are two interesting variations to consider:\n",
      "\n",
      "1. **Single $n$-grams**: contiguous sequence of $n$ items for $n=1, 2, \\dots$\n",
      "   \n",
      "2. **Simultaneous cumulative $n$-grams**: contiguous sequence of $1 \\dots n$ items simultaneously for $n=1, 2, \\dots$\n",
      "\n",
      "One assumes that the former is bound to introduce extreme sparsity while the latter may suffer from the curse of dimensionality. Nonetheless, we are motivated to experiment with these variations by the prospect of improving classification accuracy in the presence of negated phrases and contrastive conjunctions (e.g. 'not good', 'far from the best', 'on the contrary', etc.)\n",
      "\n",
      "Due to computational and obvious practical constraints (read: commonsense), we only observe $n \\leq 5$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "#pl.rcParams['figure.figsize'] = (20.0, 20.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define a vectorizer that works nicely with Twitter API data. This is currently equivalent to a  `CountVectorizer` but can be trivially extended and augmented by a `DictVectorizer`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from feature_extraction import CountDictCombinedVectorizer\n",
      "\n",
      "class TweetVectorizer(CountDictCombinedVectorizer):\n",
      "\n",
      "    def string_value(self, x):\n",
      "        return x.get(u'text')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Retrieve all the available data from *SemEval-2013 Task B*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from data import load_semeval\n",
      "twitter_data = list(load_semeval(subtask='b', subset='all'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Normalize and encode the targets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "def normalize_target(target):\n",
      "    return u'neutral' if target in ('neutral', 'objective', 'objective-OR-neutral') else target\n",
      "\n",
      "labels = [normalize_target(tweet['class']['overall']) for tweet in twitter_data]\n",
      "\n",
      "le = LabelEncoder()\n",
      "y = le.fit_transform(labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectorize the data with respect to each $n$-gram variation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from analyzer import preprocess, tokenize\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "X_dict = {}\n",
      "\n",
      "for i in xrange(1, 6):\n",
      "    for ngram_range in [(1, i), (i, i)]:\n",
      "        vec = TweetVectorizer(count_vectorizer=CountVectorizer(tokenizer=tokenize, preprocessor=preprocess, ngram_range=ngram_range))\n",
      "        X_dict[ngram_range] = vec.fit_transform(twitter_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train a multinomial logistic regression classifier (maximum entropy classifier) and generate learning curve with 5-fold cross validation with respect to each $n$-gram variation (this takes some time...)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.learning_curve import learning_curve\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "learning_curves = {}\n",
      "for ngram_range in X_dict:\n",
      "    learning_curves[ngram_range] = learning_curve(LogisticRegression(), X_dict[ngram_range], y, cv=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named learning_curve",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-56d26090c4a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_curve\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlearning_curves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named learning_curve"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first consider the simultaneous cumulative $n$-grams and their respective accuracy scores with respect to the size of the training set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = pl.subplots()\n",
      "\n",
      "for i in xrange(1, 6):\n",
      "    train_sizes, train_scores, test_scores = learning_curves[(1, i)]\n",
      "    ax.plot(train_sizes, test_scores, label='(1, {n})-gram'.format(n=i))\n",
      "    \n",
      "pl.title('Learning Curve')\n",
      "pl.xlabel('Training set size')\n",
      "pl.ylabel('Test Score')\n",
      "\n",
      "pl.xlim(train_sizes[0], train_sizes[-1])\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We notice that the performance systematically decreases as $n$ increases but ultimately converges within a reasonable range once given all the available training data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us look at the singular $n$-grams."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = pl.subplots()\n",
      "\n",
      "for i in xrange(1, 6):\n",
      "    train_sizes, train_scores, test_scores = learning_curves[(i, i)]\n",
      "    ax.plot(train_sizes, test_scores, label='{n}-gram'.format(n=i))\n",
      "    \n",
      "pl.title('Learning Curve')\n",
      "pl.xlabel('Training set size')\n",
      "pl.ylabel('Test Score')\n",
      "\n",
      "pl.xlim(train_sizes[0], train_sizes[-1])\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we observe that bigrams alone performs significantly worse than unigrams alone and again, performance systematically decreases as $n$ increases. We speculate that for 4, 5-grams, the data is so prohibitively sparse that it is incapable of generalization. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = pl.subplots()\n",
      "\n",
      "for i in xrange(1, 6):\n",
      "    train_sizes, train_scores, test_scores = learning_curves[(i, i)]\n",
      "    ax.plot(train_sizes, train_scores, label='{n}-gram'.format(n=i))\n",
      "    \n",
      "pl.title('Learning Curve')\n",
      "pl.xlabel('Training set size')\n",
      "pl.ylabel('Training Score')\n",
      "\n",
      "pl.xlim(train_sizes[0], train_sizes[-1])\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observing the learning curve with respect to performance on training data not only confirms this but raises some questions about the generalization capabilities of models trained on 2, 3-grams. For now, we can only conclude that models trained on 4, 5-grams alone exhibit high variance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "fig, ax = pl.subplots()\n",
      "\n",
      "x = np.arange(1, 6)\n",
      "y1 = [learning_curves[(i, i)][-1][-1] for i in x]\n",
      "y2 = [learning_curves[(1, i)][-1][-1] for i in x]\n",
      "\n",
      "ax.plot(x, y1, label='single')\n",
      "ax.plot(x, y2, label='simultaneous')\n",
      "\n",
      "pl.title('Accuracy vs. n-gram size')\n",
      "pl.xlabel('n-gram size')\n",
      "pl.ylabel('Score')\n",
      "\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see that the accuracy decreases negligibly as $n$ increases when using cumulative $n$-grams simultaneously and quite dramatically when using single $n$-grams. In both cases, this decrease can be attributed to the increase in variance, of which the latter is significantly more drastic."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For completeness, we have included the dimensionality of the data with respect to each value of $n$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "fig, ax = pl.subplots()\n",
      "\n",
      "x = np.arange(1, 6)\n",
      "y1 = [X_dict[(i, i)].shape[1] for i in x]\n",
      "y2 = [X_dict[(1, i)].shape[1] for i in x]\n",
      "\n",
      "ax.plot(x, y1, label='single')\n",
      "ax.plot(x, y2, label='simultaneous')\n",
      "\n",
      "pl.title('Dimensionality vs. n-gram size')\n",
      "pl.xlabel('n-gram size')\n",
      "pl.ylabel('Dimensionality')\n",
      "\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...700000-dimensional is rather staggering concept to fathom."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are the densities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils.extmath import density\n",
      "\n",
      "fig, ax = pl.subplots()\n",
      "\n",
      "x = np.arange(1, 6)\n",
      "y1 = [density(X_dict[(i, i)]) for i in x]\n",
      "y2 = [density(X_dict[(1, i)]) for i in x]\n",
      "\n",
      "ax.plot(x, y1, label='single')\n",
      "ax.plot(x, y2, label='simultaneous')\n",
      "\n",
      "pl.title('Density vs. n-gram size')\n",
      "pl.xlabel('n-gram size')\n",
      "pl.ylabel('Density')\n",
      "\n",
      "pl.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that both are of extreme but comparable sparsity. Not surprisingly, density plummets as soon as we introduce bigrams."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}